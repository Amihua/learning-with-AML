# chapter 1 基本概念

强化学习的数学基础和建模工具是马尔可夫决策过程（Markov decision process，缩写 MDP）。**<u>一个 MDP 通常由状态空间、动作空间、状态转移函数、奖励函数等组成</u>。**

==注：在本文中，大写字母表示随机变量，小写字母代表已知的观测==

### 状态（state）

在每个时刻，环境有一个**状态（state）**，可以理解为对当前时刻环境的概括。在超级玛丽的例子中，可以把屏幕当前的画面（或者最近几帧画面）看做状态。玩家只需要

知道当前画面（或者最近几帧画面）就能够做出正确的决策，决定下一步是让超级玛丽向左、向右、或是向上。因此，状态是做决策的依据。

![image-20220117180534919](C:\Users\melon\AppData\Roaming\Typora\typora-user-images\image-20220117180534919.png)

### 状态空间（state space）

指**所有可能存在状态的集合**，记作花体字母 $\mathbf{S}$。状态空间可以是离散的，也可以是连续的。状态空间可以是有限集合，也可以是无限可数集合。在超级玛丽、星际争霸、无人驾驶这些例子中，状态空间是无限集合，存在无穷多种可能的状态。围棋、五子棋、中国象棋这些游戏中，状态空间是离散有限集合，可以枚举出所有可能存在的状态（也就是棋盘上的格局）。在组合优化问题中，这种状态一般是离散的.

### 动作（action）

智能体基于当前的状态所做出的决策。在超级玛丽的例子中，假设玛丽奥只能向左走、向右走、向上跳。那么动作就是左、右、上三者中的一种。在围棋游戏中，棋盘上有 361 个位置，于是有 361 种动作，第 $i$种动作是指把棋子放到第 $i$​个位置上。动作的选取可以是确定性的，也可以是随机的。随机是指以一定概率选取一个动作，后面将会具体讨论。

### 动作空间（action space）

是指所有可能动作的集合，记作花体字母 $\mathbf{A}$ 。在超级玛丽例子中，动作空间是 $\mathbf{A}$ = *{*左*,* 右*,* 上*}*。在围棋例子中，动作空间是 $\mathbf{A}$  = *{*1*,* 2*,* 3, *· · ·* , 361*}*。动作空间可以是离散集合或连续集合，可以是有限集合或无限集合。

### 奖励（reward）==注意区分奖励（reward）和回报（return）==

是指在智能体执行一个动作之后，环境返回给智能体的一个数值。奖励往往由我们自己来定义，奖励定义得好坏非常影响强化学习的结果。比如可以这样定义，玛丽奥吃到一个金币，获得奖励 +1；如果玛丽奥通过一局关卡，奖励是 +1000；如果玛丽奥碰到敌人，游戏结束，奖励是 *−*1000；如果这一步什么都没发生，奖励就是 0。怎么定义奖励就见仁见智了。我们应该把打赢游戏的奖励定义得大一些，这样才能鼓励玛丽奥通过关卡，而不是一味地收集金币。

![image-20220117181309563](https://gitee.com/amihua/picgo/raw/master/image-20220117181309563.png)

### 状态转移（state transition）

是指智能体从当前$t$时刻的状态$s$转移到下一个时刻状态为$s$的过程。在超级玛丽的例子中，基于当前状态（屏幕上的画面），玛丽奥向上跳了一步，那么环境（即游戏程序）就会计算出新的状态（即下一帧画面）。在中国象棋例子中，基于当前状态（棋盘上的格局），红方让“车”走到黑方“马”的位置上，那么环境（即游戏规则）就会将黑方的“马”移除，生成新的状态（棋盘上新的格局）。

![image-20220117181532777](https://gitee.com/amihua/picgo/raw/master/image-20220117181532777.png)

### 策略（policy）

**策略（policy）**的意思是根据观测到的状态，如何做出决策，即如何从动作空间中选取一个动作。策略可以是确定性的，也可以是随机性的，两种都非常有用。强化学习的目标就是得到一个策略函数，在每个时刻根据观测到的状态做出决策。

![image-20220117212710657](https://gitee.com/amihua/picgo/raw/master/image-20220117212710657.png)

### 交互（interaction）

指智能体观测到环境的状态 $s$，做出动作$a$，动作会改变环境的状态$s$，环境反馈给智能体奖励 $r$以及新的状态$s'$。

### 回合（episode）

强化学习对样本数量的要求很高，即便是个简单的游戏，也需要玩上万回合游戏才能学到好的策略。Epoch 是一个类似而又有所区别的概念，常用于监督学习。一个 epoch 意思是用所有训练数据进行前向计算反向传播，而且每条数据恰好只用一次。

## 随机性来源

随机性有两个来源：策略函数与状态转移函数。

### **动作**的随机性来自于**随机决策**。

给定当前状态$s$，策略函数$\pi (a|s)$会算出动作空间$\mathbf{A}$中每个动作的$a$概率值。智能体执

行的动作是随机抽样的结果，所以带有随机性。

![image-20220117213751843](https://gitee.com/amihua/picgo/raw/master/image-20220117213751843.png)

### **状态**的随机性来自于**状态转移函数**

当状态$s$和动作$a$都被确定下来，下一个状态仍然有随机性。环境（比如游戏程序）用状态转移函数$p(s'|s,a)$计算所有可能的状态的概率，然后做随机抽样，得到新的状态。

![image-20220117213923677](https://gitee.com/amihua/picgo/raw/master/image-20220117213923677.png)

### 奖励$r$也可能是不确定的

**奖励**是状态$s_t$和动作$a_t$的函数。假设$t$时刻的奖励是$(s_t,a_t)$的函数，记作：
$$
r_t = r(s_t,a_t)
$$
基于这种假设，给定当前状态$s_t$和动作$a_t$,那么奖励$r_t$就是唯一确定的。

但更一般的，$t$时刻的奖励$r_t = r(s_t,a_t,a_{t+1})$。如果$A_t$还没被观测到，或者$(S_t,A_t)$都还没被观测到，那么$t$时刻的奖励就具有不确定性。采用$R_t = r(s_t,A_t)$或$R_t = r(S_t,A_t)$表示$t$时刻的奖励随机变量，随机性来源于位置的状态$S_t$与动作$A_t$。

![image-20220117214958317](https://gitee.com/amihua/picgo/raw/master/image-20220117214958317.png)

## **回报与折扣回报**

奖励和回报在中文上很容易有混淆，他们的英文分别是**reward**和**return**。奖励是指，每一次智能体agent与环境交互，环境产生的回应即：观测-> 动作-> 奖励。

回报则是指：

### 回报（return）

**回报**（return） 是从当前时刻开始到本回合结束的所有奖励的总和，所以回报也叫做**累计奖励**（cumulative future reward）。

**强化学习的目标就是寻找一个策略，使得回报的期望最大化。**这个策略称为最优策略 (optimum policy)。==强化学习的目标是最大化回报，而不是最大化当前的奖励。==

### 折扣回报（discounted return）

给未来的奖励做折扣。折扣回报的定义：
$$
U_t = R_t + \gamma \cdot R_{t+1}  + \gamma^2 \cdot R_{t+2} +\dots 
$$
![image-20220117215602824](https://gitee.com/amihua/picgo/raw/master/image-20220117215602824.png)

### 回报的随机性

在$t$时刻，假设我们只能观测到$s_t$状态之前的状态、动作、奖励，即：
$$
s_1,a_1,r_1,s_2,a_2,r_2,\dots,s_{t-1},a_{t-1},r_{t-1},s_t
$$
而下面这些都是随机变量
$$
A_t,R_t,S_{t+1},A_{t+1},R_{t+1},\dots
$$
而这些奖励$R_i$全都是未知的随机变量，所以$U_t$也是未知的随机变量

![image-20220120135403801](https://gitee.com/amihua/picgo/raw/master/image-20220120135403801.png)

### 有限期与无限期

![image-20220120141045134](https://gitee.com/amihua/picgo/raw/master/image-20220120141045134.png)

![image-20220120141146593](https://gitee.com/amihua/picgo/raw/master/image-20220120141146593.png)

## 价值函数（value function）$Q_{\pi}(s,a)$

价值函数的目的是，估计出当前状态下，未来**回报的期望**。

### 动作价值函数

在$t$时刻，如果我们知道了$U_t$,我们就知道我们要赢了还是输了，但是真实情况是，我们并不知道到$U_t$的值，因为在$s_t,a_t$下，$U_t$是个随机变量，上面我们已经分析过，它的随机性来源于策略是按分布随机采样的，状态是依据状态转移函数改变的.由于$U_t$是个随机变量，故而我们希望得到一个确定值，一个惯用的操作就是对$U_t$求期望。

假设我们已经观测到状态$s_t$,且做完决策，选中动作$a_t$.那么$U_t$的随机性来源于$t+1$时刻起的 所有状态和动作：
$$
S_{t+1},A_{t+1}, S_{t+2},A_{t+2},\dots,S_n,A_n
$$
对$U_t$关于变量$S_{t+1},A_{t+1}, S_{t+2},A_{t+2},\dots,S_n,A_n$求条件期望，得到：
$$
Q_{\pi}(s_t,a_t) = \mathbb{E}_{$S_{t+1},A_{t+1}, S_{t+2},A_{t+2},\dots,S_n,A_n}[U_t|S_t=s_t,A_t=a_t]
$$
期望中的$S_t$与$A_t=a_t$​是条件，意思是已经观测到$S_t$与$A_t$的值。条件期望的结果$Q_{\pi}(s_t,a_t)$被称作**动作价值函数**（action-value function）。

![image-20220120143454784](https://gitee.com/amihua/picgo/raw/master/image-20220120143454784.png)

更准确地说，应该叫“动作状态价值函数”，但是大家习惯性地称之为“动作价值函数”。

### **最优动作价值函数** (Optimal Action-Value Function)

有了动作价值函数，我们已经可以依靠**当前状态$s_t$、动作$a_t$、策略函数$\pi$**评价当前的游戏情况，但我们依旧没办法抛开策略，仅评价当前状态、动作的好坏，这时一个比较好的解决方案是，选择最好的策略，固定这样的策略，去得到动作价值函数，称为：**最优动作价值函数**
$$
Q_*(s_t,a_t)=\max_{x}Q_{\pi}(s_t,a_t),\forall a_t \in \mathscr{S},a_t \in \mathscr{A}
$$
意思就是有很多种策略函数$\pi$可供选择，而我们选择最好的策略函数：
$$
\pi^*=arg \max_{\pi} Q_{\pi}(s_t,a_t)
$$
最优动作价值函数$$Q_*(s_t,a_t)$$只依赖于$(s_t,a_t)$,而与策略$\pi$无关。

### 状态价值函数（state-value function）

在有了**最优动作价值函数**后，我们已经能很好的根据状态以及做出的动作评判收益，但是就如下棋一样，旁观者是通过局势判断谁快赢了，谁要输了，那么机器能否也做到这样呢？

我们在动作价值函数的地方，我们用期望去掉了未来未知事物的不确定性，在这里也如此，我们对动作的各种情况也求期望，也就是内积求和（求积），这样就将动作的不确定性消掉了，得到的称**状态价值函数**
$$
V_{\pi}(s_t) &=\mathbb{E}_{A_t \sim \pi(\cdot|s_t)}[Q_{\pi}(s_t,A_t)]\\
&= \sum_{a \in \mathscr{A} }\pi(a|s_t)\cdot Q_{\pi}(s_t,a)
$$
![image-20220120151129980](https://gitee.com/amihua/picgo/raw/master/image-20220120151129980.png)

