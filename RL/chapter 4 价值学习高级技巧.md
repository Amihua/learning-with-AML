# chapter 4 价值学习高级技巧

[TOC]

## 经验回放（experience replay）

经验回放的意思是把智能体与环境交互的记录（即经验）储存到一个数组里，事后反复利用这些经验训练智能体。这个数组被称为经验**回放数组（replay buffffer）**。具体来说，把智能体的轨迹划分成$(s_t,a_t,r_t,s_{t+1})$这样的四元组，存入一个数组。需要<u>人为指定数组的大小（记作 $b$）</u>。数组中只保留最近 $b$条数据；当数组存满之后，删除掉最旧的数据。数组的大小$b$是个需要调的超参数，会影响训练的结果。通常设置$b$为$10^5$~$10^6$。

![image-20220127094314422](https://gitee.com/amihua/picgo/raw/master/image-20220127094314422.png)

### 经验回放的优点

#### 经验回放的一个好处在于打破序列的相关性。

训练 DQN 的时候，每次我们用一个四元组对 DQN 的参数做一次更新。我们希望相邻两次使用的四元组是独立的。然而当智能体收集经验的时候，相邻两个四元组 $(s_t,a_t,r_t,s_{t+1})$和 $(s_{t+1},a_{t+1},r_{t+1},s_{t+2})$有很强的相关性。依次使用这些强关联的四元组训练 DQN，效果往往会很差。经验回放每次从数组里随机抽取一个四元组，用来对 DQN 参数做一次更新。**这样随机抽到的四元组都是独立的，消除了相关性。**

#### 重复利用收集到的经验

经验回放的另一个好处是重复利用收集到的经验，而不是用一次就丢弃，这样可以用更少的样本数量达到同样的表现。重复利用经验、不重复利用经验的收敛曲线通常如图所示。图的横轴是样本数量，纵轴是平均回报。

![image-20220127094607926](https://gitee.com/amihua/picgo/raw/master/image-20220127094607926.png)

> 注意“样本数量”（sample complexity）与“更新次数”两者的区别。**样本数量**是指智能体从环境中获取的奖励$r$的数量。而**一次更新**的意思是从经验回放数组里取出一个或多个四元组，用它对参数$w$做一次更新。通常来说，样本数量更重要，因为在实际应用中收集经验比较困难。比如，在机器人的应用中，需要在现实世界做一次实验才能收集到一条经验，花费的时间和金钱远大于做一次计算。相对而言，做更新的次数不是那么重要，更新次数只会影响训练时的计算量而已。

### 经验回放的局限性

- 并非所有的强化学习方法都允许重复使用过去的经验。
- 经验回放数组里的数据全都是用行为策略（behavior policy）控制智能体收集到的, 所以行为策略和目标策略是不同的，即“**异策略**。
- 有些强化学习方法如 SARSA、REINFORCE、A2C 都属于同策略。它们要求经验必须是当前的目标策略收集到的，而不能使用过时的经验。**经验回放不适用于同策略.**

### 优先经验回放（prioritized experience replay）

普通经验回放每次**均匀**抽样得到一个样本——即四元组$(s_t,a_t,r_t,s_{t+1})$，用它来更新 DQN 的参数。但是这样有一个问题，样本的重要性并不是均匀的，比如在无人驾驶场景中，有些极端环境并不常见，这样的样本价值非常高，非常稀有，所以意外情况的样本应当有更高的权重，受到更多关注。不应该同等对待正常行驶、意外情况的样本。所以优先经验回放的想法来源于**非均匀抽样**，即：给每个四元组一个权重，然后根据权重做**非均匀**随机抽样。如果 DQN 对$(s_j, a_j)$的价值判断不准确，即 $Q(s_j,a_j;w)$ 离$Q_*(s_j,a_j)$较远，则四元组 应当有较$(s_j,a_j,r_j,s_{j+1})$高的权重。

接下来我们关心的问题自然就是这个权重应该怎么确定？一个自然的想法就是：**让机器更加关注错的很离谱的数据。**![image-20220127105051915](https://gitee.com/amihua/picgo/raw/master/image-20220127105051915.png)![image-20220127105103627](https://gitee.com/amihua/picgo/raw/master/image-20220127105103627.png)![image-20220127105515712](https://gitee.com/amihua/picgo/raw/master/image-20220127105515712.png)

![image-20220127115910040](https://gitee.com/amihua/picgo/raw/master/image-20220127115910040.png)

![image-20220127120620321](https://gitee.com/amihua/picgo/raw/master/image-20220127120620321.png)

做经验回放的时候，每次取出一个（或多个）四元组，用它计算出新的 TD 误差：
$$
\delta_{j}^{'}=Q(s_j,a_j;w_{now}) - [r_t+ \gamma \cdot \max_{a \in \mathscr{A}} Q(s_{j+1,a;w_{now}})]
$$
然后用它更新 DQN 的参数。用这个新的$\delta_j^{'}$取代数组中旧的$\delta_j$

##  高估问题

Q 学习算法有一个缺陷：用 Q 学习训练出的 DQN 会高估真实的价值，而且高估通常是非均匀的。高估问题并不是 DQN 模型的缺陷，而是 Q 学习算法的缺陷。

Q 学习产生高估的原因有两个：

1. 自举导致偏差的传播；

2. 最大化导致 TD 目标高估真实价值

   为了缓解高估，需要从导致高估的两个原因下手，改进 Q 学习算法。双 Q 学习算法是一种有效的改进，可以大幅缓解高估及其危害。

