{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721531e0-6590-4988-819b-7923e83adab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通常利用torch建模需要的包\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e791c4-fa76-43cf-863f-9d3815d22c42",
   "metadata": {},
   "source": [
    "根据前面我们对深度学习任务的梳理，有如下几个超参数可以统一设置，方便后续调试时修改：\n",
    "\n",
    "- batch size\n",
    "- 初始学习率（初始）\n",
    "- 训练次数（max_epochs）\n",
    "- GPU配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c61120-9933-46ea-83d8-de021c9edbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # batch_size\n",
    "lr = 1e-4 # 学习率\n",
    "max_epochs = 100 # 最大迭代轮数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac341403-8ea1-4d8b-af5e-772f73f69709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu设置\n",
    "\n",
    "# # 方案一：使用os.environ，这种情况如果使用GPU不需要设置\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099826de-00dd-4f57-9d93-32e432a675bc",
   "metadata": {},
   "source": [
    "## 数据读入\n",
    "PyTorch数据读入是通过```Dataset```+```Dataloader```的方式完成的\n",
    "- ```Dataset```定义好数据的格式和数据变换形式\n",
    "- ```Dataloader```用iterative（迭代器）的方式不断读入批次数据。\n",
    "\n",
    "我们可以定义自己的Dataset类来实现灵活的数据读取，定义的类需要继承PyTorch自身的Dataset类。主要包含三个函数：\n",
    "\n",
    "- `__init__`: 用于向类中传入外部参数，同时定义样本集\n",
    "- `__getitem__`: 用于逐个读取样本集合中的元素，可以进行一定的变换，并将返回训练/验证所需的数据。iterative（迭代器）\n",
    "- `__len__`: 用于返回数据集的样本数\n",
    "\n",
    "如果是pytorch数据集```datasets```本身就有的数据，生成的模式如下：\n",
    "这里使用了PyTorch自带的ImageFolder类的用于读取按一定结构存储的图片数据\n",
    "- ```path```对应图片存放的目录，目录下包含若干子目录，每个子目录对应属于同一个类的图片）\n",
    "- ```data_transform```可以对图像进行一定的变换，如翻转、裁剪等操作，可自己定义\n",
    "```PYTHON\n",
    "train_data = datasets.ImageFolder(train_path, transform=data_transform)\n",
    "val_data = datasets.ImageFolder(val_path, transform=data_transform)\n",
    "```\n",
    "\n",
    "如果我们需要自定义数据集，可以参考下面模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da457f1-be1e-48b6-b053-fa050836f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, info_csv, image_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir: 图片的地址.\n",
    "            info_csv: 包含图像索引的CSV文件的路径与相应的标签.\n",
    "            image_list: 训练集和测试集文件名的txt列表地址\n",
    "            transform: 在样本上应用变换\n",
    "        \"\"\"\n",
    "        label_info = pd.read_csv(info_csv)\n",
    "        image_file = open(image_list).readlines() # 读txt，按行读\n",
    "        self.data_dir = data_dir\n",
    "        self.image_file = image_file\n",
    "        self.label_info = label_info\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:传入索引\n",
    "        Returns: 图像及其label\n",
    "            \n",
    "        \"\"\"\n",
    "        # strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "        image_name = self.image_file[index].strip('\\n')\n",
    "        \n",
    "        \n",
    "        raw_label = self.label_info.loc[self.label_info['Image_index'] == image_name]\n",
    "        label = raw_label.iloc[:,0]\n",
    "        image_name = os.path.join(self.data_dir, image_name)\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1fbdb0-e4c8-4537-af31-c2b2398d1ce1",
   "metadata": {},
   "source": [
    "构建好Dataset后，就可以使用DataLoader来按批次读入数据了，实现代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98810079-ff18-4d6a-aff9-33bfa22a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae43511-1711-48da-b23a-94caaf5e5c35",
   "metadata": {},
   "source": [
    "其中:\n",
    "\n",
    "- batch_size：样本是按“批”读入的，batch_size就是每次读入的样本数\n",
    "- num_workers：有多少个进程用于读取数据\n",
    "- shuffle：是否将读入的数据打乱\n",
    "- drop_last：对于样本最后一部分没有达到批次数的样本，使其不再参与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4676ef6-634b-4d3c-818f-adf209847521",
   "metadata": {},
   "source": [
    "##  模型构建\n",
    "PyTorch中神经网络构造一般是基于```Module```类的模型来完成的，它让模型构造更加灵活。\n",
    "\n",
    "```Module```类是 nn 模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。\n",
    "\n",
    "这里定义的 MLP 类重载了 Module 类的```init```函数和```forward```函数。它们分别用于创建模型参数和定义前向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90dbb68-7a89-418b-b76f-abbd0f67a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "  def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "    super(MLP, self).__init__(**kwargs)\n",
    "    self.hidden = nn.Linear(784, 256) # 隐藏层参数\n",
    "    self.act = nn.ReLU()              # 激活函数\n",
    "    self.output = nn.Linear(256,10)   # 输出层\n",
    "    \n",
    "# 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "  def forward(self, x):\n",
    "    o = self.act(self.hidden(x))\n",
    "    return self.output(o)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec7b9f-4baa-45c5-8c87-513f64f88c21",
   "metadata": {},
   "source": [
    "我们已经写好一个类，所以接下来我们需要实例化这个继承了nn.Module的网络类，生成一个网络实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5654f756-9b22-4b2f-a8dd-9c352e1ee4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MLP()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc85f88-ef6d-4139-b4ec-88b546f88399",
   "metadata": {},
   "source": [
    "接下来我们定义一个输入X，并把它放到网络中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d369b7-cea3-4091-8cc8-635d43d0925c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1236,  0.0890,  0.0968,  0.1827, -0.1538,  0.1610,  0.1313,  0.1407,\n",
       "         -0.1381, -0.1908],\n",
       "        [ 0.0494,  0.2237,  0.1096,  0.0623,  0.1332,  0.1412,  0.0914,  0.2664,\n",
       "         -0.0886, -0.0959]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2,784)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860ecbd-07e2-4a23-a266-02136e1b937b",
   "metadata": {},
   "source": [
    "### 使用Module自定义层\n",
    "- 不含参数的层\n",
    "\n",
    "MyLayer 类通过继承 Module 类自定义了一个**将输入减掉均值后输出**的层，并将层的计算定义在了 forward 函数里。这个层里不含模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ecbd4e-652b-46bf-acd4-a8845e9938f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b9ba64-6f60-4864-b50c-95986af5a5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MyLayer()\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ceb043-f362-44eb-a0e4-2c32994f4b98",
   "metadata": {},
   "source": [
    "- **含模型参数的层**\n",
    "\n",
    "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。\n",
    "\n",
    "**Parameter 类其实是 Tensor 的子类，如果一 个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。**所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f23452-3e48-4763-806c-c2d1f305784e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        # 每一层参数4*4，一共有三个隐藏层\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)]) \n",
    "        # 增加输出层参数\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2356a0a7-bc5b-4469-b868-4a9883e98440",
   "metadata": {},
   "source": [
    "如果采用参数字典定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e91fa852-335e-4acb-887d-102ecad99edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0536bada-f53d-4cf6-be85-c64269a29bb9",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "### 二分类交叉熵损失函数\n",
    "$$\\ell(x, y)=\\left\\{\\begin{array}{ll}\n",
    "\\operatorname{mean}(L), & \\text { if reduction }=\\text { 'mean' } \\\\\n",
    "\\operatorname{sum}(L), & \\text { if reduction }=\\text { 'sum' }\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3893238-3c30-4b97-9e20-ce5fffc58a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BCELoss(weight=None,  # `weight`:每个类别的loss设置权值\n",
    "                 size_average=None, # 不建议采用，使用reduction.`size_average`:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "                 reduce=None, # `reduce`:数据类型为bool，为True时，loss的返回是标量。\n",
    "                 reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc05fa8-acd6-4182-9065-bca236ff938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4e232b-74c5-42c8-9cfe-ff85c469fca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss损失函数的计算结果为 tensor(1.3421, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('BCELoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ece8cf-e740-43ed-97da-6334df28c792",
   "metadata": {},
   "source": [
    "### 交叉熵损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f6387-f1bf-4c70-995d-034bdd47df8e",
   "metadata": {},
   "source": [
    "$$\\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum_{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0227be1-9620-4b4f-9835-4c55738d18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.CrossEntropyLoss(weight=None, # `weight`:每个类别的loss设置权值。\n",
    "                          size_average=None, # 不建议采用，使用reduction.`size_average`:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "                          ignore_index=-100, # 忽略某个类的损失函数。\n",
    "                          reduce=None, # 不建议采用，使用reduction.数据类型为bool，为True时，loss的返回是标量\n",
    "                          reduction='mean') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80554ddb-c3fb-4338-9725-8ebd5722e7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8728, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351ca7f-d7a3-400f-8221-b94f1e742623",
   "metadata": {},
   "source": [
    "[主要的已经构建好的损失函数参数解释](https://github.com/datawhalechina/thorough-pytorch/blob/main/%E7%AC%AC%E4%B8%89%E7%AB%A0%20PyTorch%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97/3.5%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a0be6-e7ce-4583-bbba-ad36bd445227",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "优化器本质上就是解方程的工具。对于一个网络，我们想要找到最优的模型参数，以经典的resnet-50为例，它大约有2000万个系数需要进行计算，如果我们直接暴力穷举一遍参数，这种方法实施可能性基本为0，堪比愚公移山plus的难度。所以工业界一般采用BP+优化器逼近求解。\n",
    "\n",
    "优化器是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。\n",
    "\n",
    "Pytorch很人性化的给我们提供了一个优化器的库torch.optim，在这里面提供了十种优化器。\n",
    "+ torch.optim.ASGD\n",
    "+ torch.optim.Adadelta\n",
    "+ torch.optim.Adagrad\n",
    "+ torch.optim.Adam\n",
    "+ torch.optim.AdamW\n",
    "+ torch.optim.Adamax\n",
    "+ torch.optim.LBFGS\n",
    "+ torch.optim.RMSprop\n",
    "+ torch.optim.Rprop\n",
    "+ torch.optim.SGD\n",
    "+ torch.optim.SparseAdam\n",
    "\n",
    "而以上这些优化算法均继承于`Optimizer`，下面我们先来看下所有优化器的基类`Optimizer`。定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3767696-c3df-4a34-8061-6ec85be0fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):        \n",
    "        self.defaults = defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e70eb36-5059-4221-9ffc-bc6c5bbca526",
   "metadata": {},
   "source": [
    "**`Optimizer`有三个属性：**\n",
    "\n",
    "+ `defaults`：存储的是优化器的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a22326f-6698-4442-8970-419090d2d001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.1,\n",
       " 'momentum': 0.9,\n",
       " 'dampening': 0,\n",
       " 'weight_decay': 0,\n",
       " 'nesterov': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'lr': 0.1, # 学习率\n",
    " 'momentum': 0.9,  # momentum 角动量\n",
    " 'dampening': 0, # 阻尼系数\n",
    " 'weight_decay': 0, # 权值衰减\n",
    " 'nesterov': False} #  Nesterov 加速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5779f0-ab38-40c8-a6e3-14a70f2cf7dd",
   "metadata": {},
   "source": [
    "`state`：参数的缓存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44a65a-08fa-40b7-af5c-fd3f0f193b5a",
   "metadata": {},
   "source": [
    "```PYTHON\n",
    "defaultdict(<class 'dict'>, \n",
    "            {tensor([[ 0.3864, -0.0131],\n",
    "                    [-0.1911, -0.4511]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "                                                                                 [0.0052, 0.0052]])}})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d2e3b-8f62-4684-9ad7-6b58398981ea",
   "metadata": {},
   "source": [
    "`param_groups`：管理的参数组，是一个list，其中每个元素是一个字典，顺序是params，lr，momentum，dampening，weight_decay，nesterov，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768254b-cd24-4699-82de-dbde6907b1ca",
   "metadata": {},
   "source": [
    "```PYTHON\n",
    "[{'params': [tensor([[-0.1022, -1.6890],\n",
    "                     [-1.5116, -1.7846]], requires_grad=True)], \n",
    "  'lr': 1, \n",
    "  'momentum': 0, \n",
    "  'dampening': 0, \n",
    "  'weight_decay': 0, \n",
    "  'nesterov': False}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2888b-1806-484f-b5ee-f0aacd4fad8f",
   "metadata": {},
   "source": [
    "**`Optimizer`还有以下的方法：**\n",
    "\n",
    "- `zero_grad()`：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b668e57-efa0-4426-a300-f2af860b7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, set_to_none: bool = False):\n",
    "    for group in self.param_groups: # 遍历每一个网络层的参数tensor\n",
    "        for p in group['params']: \n",
    "            if p.grad is not None:  #梯度不为空\n",
    "                if set_to_none: \n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    if p.grad.grad_fn is not None:\n",
    "                        p.grad. detach_() # 从当前计算图剪下，后续不能利用梯度\n",
    "                    else:\n",
    "                        p.grad.requires_grad_(False)\n",
    "                    p.grad.zero_()# 梯度设置为0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf051c5-fe01-4491-8fed-4458134296ec",
   "metadata": {},
   "source": [
    "- `step()`：执行一步梯度更新，参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c246aa0f-4cb1-481d-a822-a8e3bdd6625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, closure): \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a6c881-f987-45da-bcbf-315b08376834",
   "metadata": {},
   "source": [
    "- `add_param_group()`：添加参数组\n",
    "- `load_state_dict()` ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练\n",
    "- `state_dict()`：获取优化器当前状态信息字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9460f4f-1471-4752-b2ff-54e85d59848f",
   "metadata": {},
   "source": [
    "### 实际操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb07c110-c85e-4abf-a0c3-1c5ced35e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight before step:\n",
      "tensor([[ 0.2549,  0.0609],\n",
      "        [-2.0199, -0.8439]])\n",
      "The grad of weight before step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb6d0e8b-ba91-4803-8272-745970c14406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight after step:\n",
      "tensor([[ 0.1549, -0.0391],\n",
      "        [-2.1199, -0.9439]])\n",
      "The grad of weight after step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cceb78f5-ef82-499a-a09b-e291e89e60b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grad of weight after optimizer.zero_grad():\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "optimizer.params_group is \n",
      "[{'params': [tensor([[ 0.1549, -0.0391],\n",
      "        [-2.1199, -0.9439]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n",
      "weight in optimizer:2243298833568\n",
      "weight in weight:2243298833568\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad60298b-7efd-404f-b17f-0a3cd20bc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[ 0.1549, -0.0391],\n",
      "        [-2.1199, -0.9439]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}, {'params': [tensor([[ 0.2742, -0.2071, -1.3786],\n",
      "        [-1.3224,  1.9979, -0.0408],\n",
      "        [ 1.1698,  0.6113,  0.3265]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0}]\n",
      "state_dict before step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39c5a769-af1d-4e33-8a8f-3faf2b738e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict after step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
      "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
      "\n",
      "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n",
      "\n",
      "defaultdict(<class 'dict'>, {tensor([[-0.7404, -0.9345],\n",
      "        [-3.0153, -1.8393]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
      "        [0.0052, 0.0052]])}})\n"
     ]
    }
   ],
   "source": [
    "# 进行50次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())\n",
    "# # 保存参数信息\n",
    "# torch.save(optimizer.state_dict(),os.path.join(r\"D:\\pythonProject\\Attention_Unet\", \"optimizer_state_dict.pkl\"))\n",
    "# print(\"----------done-----------\")\n",
    "# # 加载参数信息\n",
    "# state_dict = torch.load(r\"D:\\pythonProject\\Attention_Unet\\optimizer_state_dict.pkl\") # 需要修改为你自己的路径\n",
    "# optimizer.load_state_dict(state_dict)\n",
    "# print(\"load state_dict successfully\\n{}\".format(state_dict))\n",
    "\n",
    "# 输出最后属性信息\n",
    "print(\"\\n{}\".format(optimizer.defaults))\n",
    "print(\"\\n{}\".format(optimizer.state))\n",
    "#print(\"\\n{}\".format(optimhttps://segmentfault.com/izer.param_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9910a7-cee4-4154-94d0-44d0935a8334",
   "metadata": {},
   "source": [
    "##### 注意：\n",
    "\n",
    "1. 每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27172f3a-94b2-412d-be5f-da8fe86ff842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Moddule):\n",
    "    ···\n",
    "net = Net()\n",
    "optim = torch.optim.SGD(net.parameters(),lr=lr)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447ed24-7559-422d-a47e-134e0b559344",
   "metadata": {},
   "source": [
    "2. optimizer在一个神经网络的epoch中需要实现下面两个步骤：\n",
    "\n",
    "    1. 梯度置零\n",
    "    2. 梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e033a-e68d-46a8-8418-d41701a474cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\n",
    "for epoch in range(EPOCH):\n",
    "\t...\n",
    "\toptimizer.zero_grad()  #梯度置零\n",
    "\tloss = ...             #计算loss\n",
    "\tloss.backward()        #BP反向传播\n",
    "\toptimizer.step()       #梯度更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fcedc-d0d1-4cd2-b67e-50335bbd9838",
   "metadata": {},
   "source": [
    "## 训练与评估 流程化\n",
    "\n",
    "首先应该设置模型的状态：如果是训练状态，那么模型的参数应该支持反向传播的修改；如果是验证/测试状态，则不应该修改模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a3ef0-c484-4646-bf53-55cd5e8964ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainbin  # 训练状态\n",
    "model.eval()   # 验证/测试状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce11f93-314b-4c5b-a314-146c534e71dd",
   "metadata": {},
   "source": [
    "我们前面在DataLoader构建完成后介绍了如何从中读取数据，在训练过程中使用类似的操作即可，区别在于此时要用for循环读取DataLoader中的全部数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c0ca3-1b6f-4049-8ab5-79f439767bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in train_loader:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ef013-2587-4417-8163-623e0154c448",
   "metadata": {},
   "source": [
    "之后将数据放到GPU上用于后续计算，此处以.cuda()为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303ae1-383b-45eb-9ee0-939ad66b2207",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = data.cuda(), label.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8faf9-5f27-49a4-a201-37393c65c3b0",
   "metadata": {},
   "source": [
    "开始用当前批次数据做训练时，应当先将优化器的梯度置零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3f931-ab5a-435e-ba0d-6853e56422c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe281d-a619-427c-8313-7b55efe816be",
   "metadata": {},
   "source": [
    "之后将data送入模型中训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bb5ac-088a-4d9b-aa52-7b0ae3952eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57e3e0-67df-46e0-91ed-902dc9c83838",
   "metadata": {},
   "source": [
    "根据预先定义的criterion计算损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f67187-ac92-4962-8058-4c6993e7cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776029f-db8c-4868-9bb1-6fa5762b55f8",
   "metadata": {},
   "source": [
    "将loss反向传播回网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc33fc-6d25-4fe2-be04-1e7d02afaf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb94cc-3afe-41f2-8597-ebe45567ff64",
   "metadata": {},
   "source": [
    "使用优化器更新模型参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648210b-bd1d-4453-92d6-005a20eb21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a85c2-e59e-4be8-8268-167630be706c",
   "metadata": {},
   "source": [
    "验证/测试的流程基本与训练过程一致，不同点在于：\n",
    "\n",
    "- 需要预先设置torch.no_grad，以及将model调至eval模式\n",
    "- 不需要将优化器的梯度置零\n",
    "- 不需要将loss反向回传到网络\n",
    "- 不需要更新optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f189c9-c58b-4563-957e-2ede91856831",
   "metadata": {},
   "source": [
    "一个完整的训练过程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee230aeb-f793-42e7-bd42-03bacc98609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train() # 设置模型模式\n",
    "    train_loss = 0 # 初始化训练损失\n",
    "    for data, label in train_loader: # 每次批量读取数据\n",
    "        data, label = data.cuda(), label.cuda() # 转换成gpu模式\n",
    "        optimizer.zero_grad() # 优化器梯度清零\n",
    "        output = model(data) # 获取模型输出\n",
    "        loss = criterion(label, output) # 求出损失\n",
    "        loss.backward() # 反向传播损失\n",
    "        optimizer.step() # 更新模型参数\n",
    "        train_loss += loss.item()*data.size(0) # 累计损失\n",
    "    train_loss = train_loss/len(train_loader.dataset) # 本次Epoch平均损失\n",
    "\t\tprint('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326548d-86dd-4161-bb7c-3597650b14c2",
   "metadata": {},
   "source": [
    "对应的，一个完整的验证过程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca6a166-556f-4737-9701-34bcb0f5f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):       \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            running_accu += torch.sum(preds == label.data)\n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyG",
   "language": "python",
   "name": "pyg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
